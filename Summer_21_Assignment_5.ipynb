{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Summer_21_Assignment_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinW824/CSE-144-Applied-Machine-Learning/blob/main/Summer_21_Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IlwhV0z2nMr"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chprzdhe2nMw"
      },
      "source": [
        "# Assignment 5\n",
        "\n",
        "**DUE: Sunday July 24, 2021 11:59pm**\n",
        "\n",
        "Turn in the assignment via Canvas.\n",
        "\n",
        "To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)\n",
        "\n",
        "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).\n",
        "\n",
        "Make sure you fill in any place that says \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\", as well as your name below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jeWubXE2nMx"
      },
      "source": [
        "NAME = \"Bowen Wang\"\n",
        "STUDENT_ID = \"bwang93\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-DjjESEyXnL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv1G7ixQvN8E"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y32nQiJbSAwC"
      },
      "source": [
        "## Question 1 Bitcoin price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH1DIHT1SGOo"
      },
      "source": [
        "Bitcoin, amongst other assets like AMC and GME, has been all the rage this past year and reached an all time high of \\$63,729.5 per bitcoin. Since reaching it's high, the price has dropped by nearly a half. Analysts continue to feed the frenzy by releasing price predictions that range from from \\$500,000 to \\$9,000 per bitcoin in the next year. You will make an Recurrent neural network model to gain some insight into price prediction. Yahoo! Finance is a trusted name in free financial information and has been with us since the internet's early years. You'll be using data gathered obtained from https://finance.yahoo.com/quote/BTC-USD/history/ to train your recurrent neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1U2O-ElzqES"
      },
      "source": [
        "Run the following code cell to download the training and test data. It might take a while to download the zip file and extract it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iExiItBkdS8F"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "import zipfile\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = '1JbrxeSqazKn-WjizVDPG9VvhkXVG--D-'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('BTC-USD.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ab-5H8XVOVi"
      },
      "source": [
        "# Create pandas dataframe\n",
        "data = pd.read_csv('BTC-USD.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W_KrGElWT0y"
      },
      "source": [
        "# Plot data.head()\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0UzN1BASfgD"
      },
      "source": [
        "Let's plot the bitcoin price. First, we will make a plot of bitcoin price vs the days after September 15, 2014, the start date of this dataset. Day \"0\" indicates September 15, 2014."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjcXKOtg6jb5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Used for plotting\n",
        "\n",
        "g = sns.lineplot(x = np.linspace(1,2495,2495), y = data['High'].values.reshape(-1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upn-QIHf6mwa"
      },
      "source": [
        "Next, we plot bitcoin vs. days. But this time there are dates added to the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaUQJVYCSc-S"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Used for plotting\n",
        "\n",
        "g = sns.lineplot(x = np.linspace(1,2495,2495), y = data['High'].values.reshape(-1))\n",
        "#g.map(plt.plot, \"a\", \"v\", marker=\"o\")\n",
        "g.set(xticks=np.arange(0,2500,200))\n",
        "g.set_xticklabels(rotation=30, labels = data['Date'][0::200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHC5nm2AMwT4"
      },
      "source": [
        "### Part a) Data preprocessing (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ngjtt0MfQR"
      },
      "source": [
        "In this section you will preprocess the in order to train a recurrent neural network. We can see that there 5 columns, \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adjusted Close\", and \"Volume\". We will only use the High column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhTJ8iATNFlC"
      },
      "source": [
        "# Create a dataframe that only contains High Column.\n",
        "# Hint: it may be helpful to now cast your pruned dataframe to a numpy array.\n",
        "\n",
        "data_high = ### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ6wmmb2DJnE"
      },
      "source": [
        "\n",
        "Recall [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from sklearn. Use it to scale the data for our analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FXLabvyCEN_"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "scaler = ### YOUR CODE HERE ###\n",
        "\n",
        "data_normalized = ### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqq2K_UpNnrX"
      },
      "source": [
        "You will implement a simple autoregressive recurrent neural network using the standard tensorflow RNN architectures. An autoregressive model originates from the literature on time-series models where observations from the previous time-steps are used to predict the value at the current time step. To implemement an autoregressive model, we will simply augment the data so that \"time_steps\" number of previous days are fed to the model at the current time step in order to form a prediction. The function to augment the data is given to you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtz4UhmUNngP"
      },
      "source": [
        "\n",
        "def create_dataset(dataset, time_steps=1):\n",
        "    \"\"\"\n",
        "    Generate a dataset where the time series X[n] contains the readings for the 'time_step' previous days \n",
        "    and y contains the price for today.\n",
        "    args:\n",
        "    dataset: numpy array, the data\n",
        "    time_steps: the number of previous days to feed to RNN\n",
        "\n",
        "    returns:\n",
        "    tuple: a dataset with x[i] containing 'time_step' number of previous prices, target price for x[i]\n",
        "    \"\"\"\n",
        "    dataX, dataY = [],[]\n",
        "    for i in range(len(dataset)-time_steps-1):\n",
        "        a = dataset[i:(i+time_steps)]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + time_steps])\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtUFvFWuNmiQ"
      },
      "source": [
        "# Choose the number of time steps that the model \"looks back\"\n",
        "time_steps = ### YOUR CODE HERE ###\n",
        "\n",
        "# Produce your dataset based on the number of days the model could look back\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUmwLVKQ9PVN"
      },
      "source": [
        "# Check the shape of your dataset; should be (2495-time_steps-1, time_steps) and (2495-time_steps-1,)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKjptQ4Sa7O"
      },
      "source": [
        "### Part b) Data Partitioning (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVOui6W0Nn-v"
      },
      "source": [
        "Split data into train and test sets. Use 80\\% for training and 20\\% for testing. **Note**: you need to split the data in time (the begining 80\\% of the days from start date will be the training data and the remaining 20\\% will be test data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-lwp69bNn-v"
      },
      "source": [
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ylDfvKSRpt"
      },
      "source": [
        "For this dataset, you need to reshape the partitions for the model to be able to process them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGY8MCxWSRpv"
      },
      "source": [
        "# Reshape input to be [samples, time steps, features].\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT5ychZQTh_R"
      },
      "source": [
        "### Part c) RNN Model (20 points)\n",
        "---\n",
        "\n",
        "In this part you will create a model using an RNN layer (LSTM or GRU, unidirectional or bidirectional) and train it on your training data. You will also plot training and validation loss. Use mean squared error as your model's metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6ZFzquTh_R"
      },
      "source": [
        "Compile your model and display the summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UVNRGQlTh_S"
      },
      "source": [
        "# Build your model\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "loss = ### YOUR CODE HERE ###\n",
        "\n",
        "opt = ### YOUR CODE HERE ###\n",
        "\n",
        "metrics = ### YOUR CODE HERE ###\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy6nV7WXTh_S"
      },
      "source": [
        "\n",
        "batchsize = ### YOUR CODE HERE ###\n",
        "\n",
        "epochs =  ### YOUR CODE HERE ###\n",
        "\n",
        "# Fit model\n",
        "history = ### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymO0e_oTh_S"
      },
      "source": [
        "# Plot the Model loss\n",
        "\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZPc1zwFUWmv"
      },
      "source": [
        "### Part d) More Advanced RNN Model (20 points)\n",
        "---\n",
        "In this part you will create an RNN model with the number of layers and architerure you prefer. Train it on your training data. You will also plot training and validation loss. Again, use mean squared error as your metric. In this part, you can try different models and use different hyper-parameters and report only the best one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsDP4F6RUWmx"
      },
      "source": [
        "Compile your model and display the summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJqARE-aUWmx"
      },
      "source": [
        "# Build your model\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "loss = ### YOUR CODE HERE ###\n",
        "\n",
        "opt = ### YOUR CODE HERE ###\n",
        "\n",
        "metrics = ### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuloKr8QUWmy"
      },
      "source": [
        "batchsize = ### YOUR CODE HERE ###\n",
        "epochs =  ### YOUR CODE HERE ###\n",
        "\n",
        "# Fit model\n",
        "history =  ### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiIh66vMUWmy"
      },
      "source": [
        "# Plot the Model loss\n",
        "\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkDcoN3CUWmy"
      },
      "source": [
        "### Part e): Looking at the Predictions (10 points)\n",
        "---\n",
        "\n",
        "Now, Using the final (best) model you trained, show your model's performance on the test set. Plot the model's prediction for Bitcoin Price along with the actual test set prices. Lastly, note how your model's predictions change with your model's architecture and the number of days you \"look back\". Does your model perform better with more \"look back days\" or less. Did adding more layers help? Does your model use dropout or batchnormalization?\n",
        "\n",
        "**Note:** Your model is trained on normalized data. Inorder to transform your model's predictions to the original price range you will likely need to use sklearn's inverse_transform (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7dZ2Of1R4kI"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1to7eXUQNbDd"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3iSKUeej-zA"
      },
      "source": [
        "## Question 3: Denoising Autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agjvMlaJCxjo"
      },
      "source": [
        "You will now build a simple denoising autoencoder using the MNIST Fashion dataset. Begin by getting our imports and downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWSpcNIzv5hH"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Data visualizaton.\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "import random as rn\n",
        "rn.seed(123)\n",
        "np.random.seed(seed=123)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE9HJtpmZh-F"
      },
      "source": [
        "# Plot some of the images from x_test\n",
        "fig, ax = plt.subplots(5, 4)\n",
        "fig.set_size_inches(15, 15)\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(4):\n",
        "        l=rn.randint(0, len(y_test))\n",
        "        ax[i, j].imshow(x_test[l], cmap='gray')\n",
        "        ax[i, j].set_title('Label: ' + str(y_test[l]))\n",
        "        # Hide grid lines\n",
        "        ax[i, j].grid(False)\n",
        "        # Hide axes ticks\n",
        "        ax[i, j].set_xticks([])\n",
        "        ax[i, j].set_yticks([])\n",
        "        \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Voahh8ByazLv"
      },
      "source": [
        "As expected, we have the MNIST Fashion dataset. Now, noise will be added to the images in boath x_train and x_test. Then, we will plot some of the noisy images from x_train. This may take a little while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpX-mNNWdDW_"
      },
      "source": [
        "def corrupt_pixel(image):\n",
        "    \"\"\"selects a certain number of pixels (between 50 and 100) and changes their value to 255\"\"\"\n",
        "    num_pixels = np.random.randint(50,100)\n",
        "    img = image.copy()\n",
        "\n",
        "    for i in range(num_pixels):\n",
        "        cell = np.random.randint(0,28,size = 2)\n",
        "        img[cell[0]][cell[1]] = np.random.randint(125,255)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfIQ2VhsZhtn"
      },
      "source": [
        "# corrupt the images\n",
        "x_train_noisy = [] #x_train with noise added\n",
        "for image in x_train:\n",
        "    x_train_noisy.append(corrupt_pixel(image))\n",
        "\n",
        "x_test_noisy = [] #x_test with noise added\n",
        "for image in x_test:\n",
        "    x_test_noisy.append(corrupt_pixel(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koQMPCpYZhke"
      },
      "source": [
        "# Replot some of the test images with noise added\n",
        "rn.seed(123)\n",
        "np.random.seed(seed=123)\n",
        "\n",
        "fig, ax = plt.subplots(5, 4)\n",
        "fig.set_size_inches(15, 15)\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(4):\n",
        "        l=rn.randint(0, len(y_test))\n",
        "        ax[i, j].imshow(x_test_noisy[l], cmap='gray')\n",
        "        ax[i, j].set_title('Label: ' + str(y_test[l]))\n",
        "        # Hide grid lines\n",
        "        ax[i, j].grid(False)\n",
        "        # Hide axes ticks\n",
        "        ax[i, j].set_xticks([])\n",
        "        ax[i, j].set_yticks([])\n",
        "        \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDGQ3eOaj4dJ"
      },
      "source": [
        "We can now see that we have corrupted the images. We will now build a denoising autoencoder to hopefully remove some of the noise. First, data will be normalize the data and an extra dimension will be added to the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J9LHYT5kf86"
      },
      "source": [
        "# Scale X_train to 255. which is max\n",
        "x_train  = np.array(x_train)/255.\n",
        "\n",
        "# Reshape X_train to (N,28,28,1) for convolution layers\n",
        "x_train = x_train.reshape(len(x_train),28,28,1)\n",
        "\n",
        "# Scale X_test to 255. which is max\n",
        "x_test  = np.array(x_test)/255.\n",
        "\n",
        "# Reshape X_train to (N,28,28,1) for convolution layers\n",
        "x_test = x_test.reshape(len(x_test),28,28,1)\n",
        "\n",
        "# Scale X_train to 255. which is max\n",
        "x_train_noisy  = np.array(x_train_noisy)/255.\n",
        "\n",
        "# Reshape X_train to (N,28,28,1) for convolution layers\n",
        "x_train_noisy = x_train_noisy.reshape(len(x_train_noisy),28,28,1)\n",
        "\n",
        "# Scale X_test to 255. which is max\n",
        "x_test_noisy  = np.array(x_test_noisy)/255.\n",
        "\n",
        "# Reshape X_train to (N,28,28,1) for convolution layers\n",
        "x_test_noisy = x_test_noisy.reshape(len(x_test_noisy),28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJxWpFG-8DnD"
      },
      "source": [
        "### Part a) Build the autoencoder (20 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KixYF61_C4kB"
      },
      "source": [
        " Now we will build the autoencoder. You are free to build an architecture of your choice. Again, You are welcome to use any code from previous class exercises, section handouts, and lectures. Build your model below and comment on your model's architecture. Do you have convolution layers, or a fully connected encoder, dropout and/or batch normalization, what kind of activation did you use? It may help to think about the encoder/decoder architectures simultaneously.\n",
        "\n",
        "You may use the TensorFlow documentation freely. You might also find online tutorials helpful. However, all code that you submit must be your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPZtzNoWoBs-"
      },
      "source": [
        "# Build your model\n",
        "\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKW__2rBvKRo"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0LytJav9n1"
      },
      "source": [
        "### Part b) Training the autoencoder (10 points)\n",
        "Write code that trains your autoencoder. Use mean square error as your loss, choose an optimizer of your choice. Be\n",
        "\n",
        "Plot the training curve as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNtqonrfza3E"
      },
      "source": [
        "# Compile your model\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Train the autoencoder\n",
        "\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vErU2GXOw1qL"
      },
      "source": [
        "# Plot the training curve for loss\n",
        "\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idxeQgj5wvzS"
      },
      "source": [
        "### Part c) Plot images and comment (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtbP4k7oxZtJ"
      },
      "source": [
        "Plot images some of the \"denoised\" images from the test set. Comment on how your model has performed. Was your model able to denoise the image successfully? Is there anything unexpected results about the images? MSE error is known to smooth an image too much, did your model suffere the same result?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYWjzq30xLR2"
      },
      "source": [
        "# Plot images some of the \"denoised\" images from the test set\n",
        "\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0OhDL1dLyGY"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    }
  ]
}