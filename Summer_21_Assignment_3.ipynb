{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summer_21_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinW824/CSE-144-Applied-Machine-Learning/blob/main/Summer_21_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IlwhV0z2nMr"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU7xWQml74JY"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chprzdhe2nMw"
      },
      "source": [
        "# Assignment 3\n",
        "\n",
        "**DUE: Sunday July 11 at 11:59pm**\n",
        "\n",
        "Turn in the assignment via Canvas.\n",
        "\n",
        "To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)\n",
        "\n",
        "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Runtime→→Restart runtime) and then run all cells (in the menubar, select Runtime→→Run All).\n",
        "\n",
        "Make sure you fill in any place that says \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\", as well as your name below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jeWubXE2nMx"
      },
      "source": [
        "NAME = \"\"\n",
        "STUDENT_ID = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_22iEs6s1oL"
      },
      "source": [
        "## Problem 1 - Logistic Regression Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrpQWvSs0x3v"
      },
      "source": [
        "Recall from lecture sigmoid function:\n",
        "$$\\begin{align}\n",
        "f(x)= \\sigma(\\theta^T \\cdot \\mathbf{x})) = \\frac{1}{1+e^{\\theta^T \\cdot \\mathbf{x}}}\n",
        "\\end{align}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeJAjBYEs6iJ"
      },
      "source": [
        "### a) Probabilities (10 points)\n",
        "Write a script to, or by hand, calculate the probability $ \\sigma(\\theta^T \\cdot \\mathbf{x}))$ for each of the following $x^i$'s assuming Assuming $\\theta=[-0.1, 0.1, 0.5, 0.3]$, \n",
        "\n",
        "| | $x^i_{0}$ | $x^i_{1}$ | $x^i_{2}$ | y|\n",
        "| --- | --- | --- | --- | --- |\n",
        "|$x^{1}$ | 1 | -7 | -3 | 0 |\n",
        "|$x^{2}$ | 1 | 5 | 1 | 1 |\n",
        "|$x^{3}$ | 1 | 1 | 1 | 1 |\n",
        "|$x^{4}$ | -1 | -1 | 1 | 1 |\n",
        "|$x^{4}$ | 2 | -1 | 3 | 0 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3AQpCM_BE-M"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd413HoUtBRH"
      },
      "source": [
        "### b) Classification (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmoTCICMBMGs"
      },
      "source": [
        "Using the probabilities you calculated in part a) and the decision boundary $\\theta^T\\mathbf{x}=0$ classify the points as class 1 or class 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nKSXta9BIW7"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IeQoSWbtEPn"
      },
      "source": [
        "## Problem 2 - Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh57Se9YtZBD"
      },
      "source": [
        "For this problem, we will predict whether or not breast tumors are cancerous or not using the Breast Cancer Wisconsin (Diagnostic) Data Set. Begin by importing and processing data. A description of the dataset is given below.\n",
        "\n",
        "\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. \n",
        "\n",
        "Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\n",
        "\n",
        "The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
        "\n",
        "This database is also available through the UW CS ftp server:\n",
        "ftp ftp.cs.wisc.edu\n",
        "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
        "\n",
        "Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1) ID number\n",
        "\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "\n",
        "3 to 32)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "* a) radius (mean of distances from center to points on the perimeter)\n",
        "* b) texture (standard deviation of gray-scale values)\n",
        "* c) perimeter\n",
        "* d) area\n",
        "* e) smoothness (local variation in radius lengths)\n",
        "* f) compactness (perimeter^2 / area - 1.0)\n",
        "* g) concavity (severity of concave portions of the contour)\n",
        "* h) concave points (number of concave portions of the contour)\n",
        "* i) symmetry\n",
        "* j) fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three\n",
        "largest values) of these features were computed for each image,\n",
        "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
        "13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "Class distribution: 357 benign, 212 malignant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qm6WAqNPRRu"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyKWchKWIvPi"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwexIqp8I_lF"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLF5xucumaai"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1igyRhO_Vugc0WD_bop9WUqX61Q4BJYBg\"})\n",
        "downloaded.GetContentFile('data.csv')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpxch0eIm2D3"
      },
      "source": [
        "# Create pandas dataframe\n",
        "data = pd.read_csv('data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEGGgoy6nD4w"
      },
      "source": [
        "# Let's look at the data\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8birwmMhNxqx"
      },
      "source": [
        "# drop Unnamed: 32 column\n",
        "data = data.iloc[:,0:-1]\n",
        "\n",
        "#drop the id column\n",
        "data = data.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roig66zZ4z3B"
      },
      "source": [
        "# Let's check the data again\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PRmp0GI2nOi"
      },
      "source": [
        "### a) Converting Categorical Data to Numeric Values (5 points)\n",
        "\n",
        "The diagnosis column has categorical values 'B' if a tumor is benign, and 'M' if it is malignant. Convert these values to 0 for 'B' and 1 for 'M'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0vh80Yc2nOj"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zsToX3tPahn"
      },
      "source": [
        "### b) Run Logistic Regression (10 points)\n",
        "Divide your data into feature set X which contains the following variables:\n",
        "* radius_mean\t\n",
        "* texture_mean\t\n",
        "* perimeter_mean\tarea_mean\t\n",
        "* smoothness_mean\t\n",
        "* compactness_mean\n",
        "* concavity_mean\t\n",
        "* concave points_mean\t\n",
        "* symmetry_mean\t\n",
        "* fractal_dimension_mean\tradius_se\t\n",
        "* texture_se\tperimeter_se\t\n",
        "* area_se\t\n",
        "* smoothness_se\t\n",
        "* compactness_se\t\n",
        "* concavity_se\t\n",
        "* concave points_se\t\n",
        "* symmetry_se\t\n",
        "* fractal_dimension_se\t\n",
        "* radius_worst\t\n",
        "* texture_worst\t\n",
        "* perimeter_worst\t\n",
        "* area_worst\tsmoothness_worst\t\n",
        "* compactness_worst\t\n",
        "* concavity_worst\t\n",
        "* concave points_worst\t\n",
        "* symmetry_worst\t\n",
        "* fractal_dimension_worst\n",
        "\n",
        "and labels y which contains the diagnosis column. Then divide your data into 75% training set data and 25% test set data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8e_w2-rRtED"
      },
      "source": [
        "# Split data into X and y\n",
        "X = ### YOUR CODE HERE ###\n",
        "y = ### YOUR CODE HERE ###\n",
        "\n",
        "# Split X and y into X_train, y_train, X_test, y_test using train_test_split()\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHlyB20UPqQx"
      },
      "source": [
        "# instantiate LinearRegression\n",
        "clf = ### YOUR CODE HERE ### \n",
        "\n",
        "\n",
        "# Fit the regressor using X_train and y_train\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlmjRX-FV6Fa"
      },
      "source": [
        "### c) Classification accuracy for the training and test sets (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfwZS4CJWNrT"
      },
      "source": [
        "# Generate predictions using X_train\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Calculate the classification accuracy for the training set\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Generate predictions using X_test\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Calculate the classification accuracy for the test set\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56vnLVWyU83B"
      },
      "source": [
        "### d) False Positives, False Negatives, True Positives, True Negatives (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luz4lriJdESy"
      },
      "source": [
        "In binary classification, A **false positive** (FP) is an outcome where the model incorrectly predicts the positive class. And a **false negative** (FN) is an outcome where the model incorrectly predicts the negative class. Likewise, a **true positive** (TP) is an outcome where the model correctly predicts a positive class. A **true negative** (TN) is an outcome where the model correctly predicts the negative class.\n",
        "\n",
        "Calculate the number of FPs, FNs, TPs,and TNs using the test set predictions and test set labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLFPG2AbhGj0"
      },
      "source": [
        "# Calculate the number of false positives using the test set predictions and test set labels.\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Calculate the number of false negatives using the test set predictions and test set labels.\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "#Calculate the number of true positives using the test set predictions and test set labels.\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "#Calculate the number of true negatives using the test set predictions and test set labels.\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p_GEkAbjYHv"
      },
      "source": [
        "### e) Precision and Recall (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0xed4wIjcmn"
      },
      "source": [
        "Two important statistics used to analyze the performance of classification models are precision and recall. **precision** (also called positive predictive value) is proportion of positive identifications that were actually correct, while **recall** (also known as sensitivity) is the proportion of actual positives that were identified correctly.\n",
        "\n",
        "Precision can be calculated as:\n",
        "$$Precision = \\frac{TP}{TP+FP}$$\n",
        "\n",
        "Recall can be calculated as:\n",
        "$$Recall = \\frac{TP}{TP+FN}$$\n",
        "\n",
        "For more info: https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THYqeYZ5o_lE"
      },
      "source": [
        "Use your answer from part d) to calculate Precision:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMym2_FYpM3u"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTdTM4vmpMxc"
      },
      "source": [
        "User your answer from part d) to calculate Recall:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5gUHq9MpbVG"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZITT5VUttzO"
      },
      "source": [
        "## Problem 3 - Neural Network For Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMCxB72z-kC9"
      },
      "source": [
        "### a) Training a simply neural network for classification (5 points)\n",
        "Complete the following code segments to create a NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt7K04ckhK1v"
      },
      "source": [
        "# import Keras from TensorFlow\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1Ekoez3ciwE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets.samples_generator import make_moons\n",
        "from pylab import rcParams\n",
        "\n",
        "# Create the training data\n",
        "np.random.seed(42) \n",
        "data, labels = make_moons(n_samples=500, noise=0.1)\n",
        "colors = ['r' if y else 'b' for y in labels]\n",
        "print('data.shape =', data.shape)\n",
        "print('labels.shape =', labels.shape)\n",
        "plt.scatter(data[:,0], data[:,1], c=colors)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW6XzU_IciwH"
      },
      "source": [
        "# Create and plot the test data\n",
        "np.random.seed(17)   \n",
        "test_data, test_labels = make_moons(n_samples=500, noise=0.1)\n",
        "colors = ['r' if y else 'b' for y in test_labels]\n",
        "print('test_data.shape =', test_data.shape)\n",
        "print('test_labels.shape =', test_labels.shape)\n",
        "plt.scatter(test_data[:,0], test_data[:,1], c=colors)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ1Coz15ciwJ"
      },
      "source": [
        "The above image is created by sampling from the same distribution as before. But these are entirely different points than your model was trained on.  So how our model performs on this test data will be a good indication of our model's ability to generalize.\n",
        "\n",
        " In the cell below, construct a simple neural network with 5 layers as follows:\n",
        "* **input layer** of shape 5\n",
        "* **dense layer** with 10 neurons, and relu activation\n",
        "* **dense layer** with 4 neurons, and relu activation\n",
        "* **dense layer** with 3 neurons, and relu activation\n",
        "* **dense layer** with 1 neuron, and sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRRrqpb1ciwK"
      },
      "source": [
        "def build_model1():\n",
        "    ### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k6KzH-yXOAn"
      },
      "source": [
        "# Get model summary\n",
        "model = build_model1()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PROyV-LvciwL"
      },
      "source": [
        "# Compile the NN model, defining the optimizer to use (sgd), the loss function (binary_crossentropy), and the metrics (acc) to use.\n",
        "# These settings are appropriate for a binary classification task.\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='sgd',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Train the model (obviously on the training data), iterating on the data in batches of 32 samples for 300 epochs. Have a validation_split of 0.2.\n",
        "history = ### YOUR CODE HERE ###\n",
        "\n",
        "# Evaluate the model's performance\n",
        "train_loss, train_acc = model.evaluate(data, labels)\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print('Training set accuracy:', train_acc)\n",
        "print('Test set accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nq6RMLzciwN"
      },
      "source": [
        "Let's next look at some training plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te_BySHr13P2"
      },
      "source": [
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpoIfuW8ciwQ"
      },
      "source": [
        "# The history of our accuracy during training.\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Number of epochs')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkivLoq-ciwU"
      },
      "source": [
        "# The history of our cross-entropy loss during training.\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Number of epochs')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXPkBbXs7Uqa"
      },
      "source": [
        "### b) Analyzing model performance (5 points)\n",
        "What was your model's final prediction accuracy on the test set? Explain the pattern/trend you have observed in the previous two plots in your own words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW4NHPHi78Fu"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-UNwaIt_HgR"
      },
      "source": [
        "### c) Decision Boundary (5 points)\n",
        "Plot the decisin boundary of the network you built (using the code below) and explain what you observe and why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYDz8YyPciwW"
      },
      "source": [
        "The following code will allow us to visualize our deep neural network's decision boundary. Let's observe.\n",
        "\n",
        "This will take a moment to construct the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs6FGaHKciwX"
      },
      "source": [
        "#\n",
        "# This code is substantively from https://rohitmidha23.github.io/Neural-Network-Decision-Boundary/\n",
        "#\n",
        "def plot_decision_boundary(X, y, model, steps=1000, cmap='bwr'):\n",
        "    # The following allows you to adjust the plot size\n",
        "    rcParams['figure.figsize'] = 8, 6  # 8 inches by 6 inches\n",
        "    cmap = plt.get_cmap(cmap)\n",
        "\n",
        "    # Define region of interest by data limits\n",
        "    xmin, xmax = X[:,0].min() - 1, X[:,0].max() + 1\n",
        "    ymin, ymax = X[:,1].min() - 1, X[:,1].max() + 1\n",
        "    x_span = np.linspace(xmin, xmax, steps)\n",
        "    y_span = np.linspace(ymin, ymax, steps)\n",
        "    xx, yy = np.meshgrid(x_span, y_span)\n",
        "\n",
        "    # Make predictions across region of interest\n",
        "    labels = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Plot decision boundary in region of interest\n",
        "    z = labels.reshape(xx.shape)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.contourf(xx, yy, z, cmap=cmap, alpha=0.5)\n",
        "\n",
        "    # Get predicted labels on training data and plot\n",
        "    train_labels = model.predict(X)\n",
        "    ax.scatter(X[:,0], X[:,1], c=y, cmap=cmap, lw=0)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "plot_decision_boundary(test_data, test_labels, model) \n",
        "# Reset figure size back to default.\n",
        "rcParams['figure.figsize'] = 6, 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLtd20xC2Mj3"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak3Y2clPLZpv"
      },
      "source": [
        "## Problem 4 - Fish Data revisited "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wiXAk_ILy2N"
      },
      "source": [
        "We will revist the \"Fish\" dataset and train a regression neural network on the features \"Length1\", \"Length2\", \"Length3\",\t\"Height\", and \"Width\" to predict Weight. Run or fill in the following cells and answer written response questions for each section. Begin by downloading \"Fish.csv\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMy-21iKABVc"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTvM0_JzABVc"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLDSqS7_JD0j"
      },
      "source": [
        "# Download the Fish data\n",
        "downloaded = drive.CreateFile({'id':\"1AtMi-xCejVlhYS5qjgjjW4gH-TLuWJjC\"})\n",
        "downloaded.GetContentFile('Fish.csv')  \n",
        "\n",
        "# Create pandas dataframe\n",
        "fish_data = pd.read_csv('Fish.csv')\n",
        "\n",
        "# Delete any rows for which there is a measurement of 0.0.\n",
        "fish_data = fish_data.drop( np.where(fish_data==0)[0] )\n",
        "fish_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IZwN2j4yIaX"
      },
      "source": [
        "fish_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH5esQZKdLNX"
      },
      "source": [
        "### a) Data Preprocessing (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yGF14XXidKS"
      },
      "source": [
        "Recall from the previous lectures and class excercises that it is especially important to normalize data for neural networks.\n",
        "\n",
        "In order to normalize our data to $[0,1]$ we use the equation:\n",
        "\n",
        "$$x_{norm}=\\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
        "\n",
        "Normalize the \"Length1\", \"Length2\", \"Length3\",\t\"Height\", and \"Width\", and \"Weight\" columns. We often need to normalize the target variables in a neural network regression task to limit exploding gradients (why might exploding gradients be more prevalent in a regression task?).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA98AUX0kukU"
      },
      "source": [
        "# Get fish max and min weight, we'll need these later.\n",
        "fish_max = fish_data[\"Weight\"].max()\n",
        "fish_min = fish_data[\"Weight\"].min()\n",
        "\n",
        "# Normalize the \"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\", \"Weight\" columns of fish_data.\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "# Take a look at the new age column.\n",
        "fish_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCtWDzuDoI2J"
      },
      "source": [
        "# split the data into features X_fish and target variable y_fish.\n",
        "y_fish = fish_data.iloc[:, 1] # Get Fish Weights\n",
        "X_fish = fish_data.drop(columns=['Weight']) # Get Fish measurements plus species\n",
        "X_fish = X_fish.drop(columns=['Species']) # Drop the Fish Species for now\n",
        "\n",
        "# print X.head(), you should have 5 features for each sample\n",
        "print(\"X_fish.head():\")\n",
        "print(X_fish.head())\n",
        "\n",
        "# print y.head(), you should have one label for each sample\n",
        "print(\"\\ny_fish.head()\")\n",
        "print(y_fish.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr_6JsI5oyRl"
      },
      "source": [
        "# Split the X_fish and y_fish into 60/20 training/test split using train_test_split()\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSCV-qq2oUyk"
      },
      "source": [
        "### b) Neural Network Training (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEpeCVuNvK4I"
      },
      "source": [
        "In the cell below, build a neural network 4 layers as follows:\n",
        "* **input layer** of shape 5\n",
        "* **dense layer** with 10 neurons, and sigmoid activation\n",
        "* **dense layer** with 10 neurons, and sigmoid activation\n",
        "* **dense layer** with 5 neurons, and sigmoid activation\n",
        "* **dense layer** with 1 neuron, and linear activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pno8UsUpSmR"
      },
      "source": [
        "def build_model1():\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUJtdeuqkxa"
      },
      "source": [
        "# Get model summary\n",
        "model = build_model1()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtfFKunWp0jI"
      },
      "source": [
        "Declare an SGD optimizer with learning rate of 0.05, weight decay of 1e-6 and momentum of 0.9. Compile your model Use mean_squared_error as the loss and metrics with the sgd optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok2LdYJfpr05"
      },
      "source": [
        "# Declare the optimizer\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Compile model\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQVlE8NkXOAv"
      },
      "source": [
        "Perform model fitting using the training set. Train for 2500 epochs with a batch size of 128 and a validation split of 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV6j3uRfqN-4"
      },
      "source": [
        "# Fit model\n",
        "history = ### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0HqYFKtrVOM"
      },
      "source": [
        "# Get fish weight predictions\n",
        "y_fish_pred = model.predict(x_test)\n",
        "\n",
        "# map normalized fish weights back to grams\n",
        "y_fish_pred = (fish_max-fish_min) * y_fish_pred + fish_min \n",
        "y_test = (fish_max-fish_min) * y_test.values + fish_min "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0cyLYP-qx36"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Compute the mean squared error using y_fish_test and y_fish_pred\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Compute the coefficient of determination using y_fish_test and y_fish_pred\n",
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXh5nXxgzAsZ"
      },
      "source": [
        "# Print the predictions along with actual weights\n",
        "np.set_printoptions(formatter={'float_kind':'{:f}'.format},precision=2)\n",
        "print(np.concatenate((y_fish_pred.reshape(len(y_fish_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FprpdF_YzJf6"
      },
      "source": [
        "What was the R-squared value and mean squared error for this model? Does this model appear to do a better job at predicting weights than linear regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDRR8wzy0SZu"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    }
  ]
}